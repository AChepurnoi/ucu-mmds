{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "try:\n",
    "    findspark.init()\n",
    "except:\n",
    "    PYSPARK_PATH = '../spark/spark-2.4.3-bin-hadoop2.7/' # change path to yours\n",
    "    findspark.init(PYSPARK_PATH)\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import UserDefinedFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "stages = [\"1_data_collection\", \"2_feature_engineering\", \"3_modeling\", \"4_evaluation\"]\n",
    "for stage in stages:\n",
    "    sys.path.insert(0, stage)\n",
    "\n",
    "from xml_to_csv import process_dumps\n",
    "from functions import filter_columns, extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure for sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"sample_data\"\n",
    "DATE = \"20190701\"\n",
    "XML_DIR = os.path.join(DATA_DIR, \"xml\")\n",
    "CSV_DIR = os.path.join(DATA_DIR, \"csv\")\n",
    "\n",
    "DUMP_BASE_URL = \"https://dumps.wikimedia.org/enwiki/{}\".format(DATE)\n",
    "\n",
    "dump_names = [\"enwiki-20190701-pages-articles-multistream14.xml-p7697599p7744799\"]\n",
    "dump_ext = \".bz2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:\n",
      "\t> enwiki-20190701-pages-articles-multistream14.xml-p7697599p7744799...\n"
     ]
    }
   ],
   "source": [
    "!rm $DATA_DIR/xml/* 2> null\n",
    "for dump_name in dump_names:\n",
    "    print(\"Loading {}...\".format(dump_name))\n",
    "    !wget -P $DATA_DIR/xml/ $DUMP_BASE_URL/$dump_name$dump_ext 2> /dev/null\n",
    "    !bzip2 -d $DATA_DIR/xml/$dump_name$dump_ext 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing XML + Fetching ORES...\n",
      "XML Files found: enwiki-20190701-pages-articles-multistream14.xml-p7697599p7744799\n",
      "Processing enwiki-20190701-pages-articles-multistream14.xml-p7697599p7744799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0494d6a15d84acfb11dbf3b2f54883d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9258348ad144260bfca3e68fa05519b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: Exception fetching ORES score for revisions. Ex: 'score'\n",
      "Alert: Exception fetching ORES score for revisions. Ex: 'score'\n",
      "Wiki dump(s) with ORES collected in sample_data/csv\n"
     ]
    }
   ],
   "source": [
    "print('Parsing XML + Fetching ORES...')\n",
    "process_dumps(XML_DIR, CSV_DIR, jupyter=True)\n",
    "!rm $DATA_DIR/xml/* 2> null\n",
    "print('Collected wiki dump(s) with ORES in {}/csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the DataFrame: 12484 records\n",
      "root\n",
      " |-- Stub: double (nullable = true)\n",
      " |-- Start: double (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- GA: double (nullable = true)\n",
      " |-- FA: double (nullable = true)\n",
      " |-- n_words: double (nullable = false)\n",
      " |-- n_internal_links: double (nullable = false)\n",
      " |-- n_external_links: double (nullable = false)\n",
      " |-- level2: double (nullable = false)\n",
      " |-- level3: double (nullable = false)\n",
      " |-- level4: double (nullable = false)\n",
      " |-- level5: double (nullable = false)\n",
      " |-- level6: double (nullable = false)\n",
      " |-- book_citations: double (nullable = false)\n",
      " |-- journal_citations: double (nullable = false)\n",
      " |-- n_paragraphs: double (nullable = false)\n",
      " |-- n_unreferenced: double (nullable = false)\n",
      " |-- n_categories: double (nullable = false)\n",
      " |-- n_images: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_raw = glob(os.path.join(CSV_DIR, \"enwiki-{}-pages-articles-multistream*_raw.csv\".format(DATE)))\n",
    "csv_feature = os.path.join(CSV_DIR, \"enwiki-{}-features.csv\".format(DATE))\n",
    "\n",
    "df = spark.read.csv(csv_raw, inferSchema=True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "df_features = extract_features(df)\n",
    "\n",
    "print(\"Size of the DataFrame: {} records\".format(df.count()))\n",
    "df_features.printSchema()\n",
    "\n",
    "df_features.toPandas().to_csv(csv_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
