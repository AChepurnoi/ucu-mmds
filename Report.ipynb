{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity of Wikipedia article references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining massive databases course final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation and goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diversity of resources and content matters. When people are obtaining new knowledge, they don't want to be fooled with fake news or believe in information without proof from other authorized source. Sometimes Wikipedia articles may have poorly filled or unreferenced information, so, in the era of exponential data growth and post-truth, there is a huge need in automatic detection of articles with deficit of sources.\n",
    "\n",
    "\n",
    "Our solution may help readers — to be more confident or sceptic about the information gained, as well as editors — such that they concentrate on the most important gaps of the article.\n",
    "\n",
    "Moreover, noteworthy that there exist differences in the same article across different languages, that can be detected with our solution and fixed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the quality of the article, based on the references, in an unsupervised way. Unite and check the results of our modeling with results from ORES model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![problem_statement](assets/clusters_w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First we'll import prepared modules:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"1_data_collection\")\n",
    "sys.path.insert(0, \"2_feature_engineering\")\n",
    "sys.path.insert(0, \"3_modeling\")\n",
    "from xml_to_csv import process_dumps\n",
    "from csv_to_features import create_features\n",
    "from features_to_clusters import get_clusters\n",
    "from test import test_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the workin pipeline is this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](assets/pipeline_w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download wikipedia XML dumps:\n",
    "    * We are using page article multistream dumps. To get faster development loops so far we worked with a single dump, next we will run the full pipeline on the whole wikipedia data. <br> <br>\n",
    "\n",
    "* Parse XML to CSV using streaming XML parser:\n",
    "    * We are using lxml and handwritten parser that goes through the file tag by tag and parses articles and meta information and article and last revision. The data we are fetching includes article text, title, revision author, revision comment and timestamp.\n",
    "\n",
    "* Fetch ORES assessments:\n",
    "    * **ORES (Objective Revision Evaluation Service)** provides score that represents article quality. The score itself consists of probabilities tha the article is:\n",
    "        * **FA** (Featured Article)\n",
    "        * **A** (A-class, well organized and essentially complete.)\n",
    "        * **GA** (Good Article)\n",
    "        * **B** (B-class, mostly complete and without major problems, but requires some further work)\n",
    "        * **C** (C-class, substantial, but is still missing important content or contains much irrelevant material)\n",
    "        * **Start** (Developing, quite incomplete; might or might not cite adequate reliable sources)\n",
    "        * **Stub** (A very basic description of the topic / very-bad-quality article)\n",
    "    * Here we use mwapi and ORES web service to get article scores\n",
    "\n",
    "* Inspect internal structure of text:\n",
    "    * Wikipedia articles have its own syntax for declaring blocks inside article: [source](https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*See data processing result for a sample data below:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enwiki-20190701-pages-articles-multistream14.xml-p7697599p7744799...\n",
      "Parsing XML + Fetching ORES...\n",
      "XML Files found: \n",
      "Collected wiki dump(s) with ORES in sample_data/csv\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"sample_data\"\n",
    "DATE = \"20190701\"\n",
    "XML_DIR = os.path.join(DATA_DIR, \"xml\")\n",
    "CSV_DIR = os.path.join(DATA_DIR, \"csv\")\n",
    "\n",
    "DUMP_BASE_URL = \"https://dumps.wikimedia.org/enwiki/{}\".format(DATE)\n",
    "\n",
    "dump_names = [\"enwiki-20190701-pages-articles-multistream14.xml-p7697599p7744799\"]\n",
    "dump_ext = \".bz2\"\n",
    "\n",
    "!rm $DATA_DIR/xml/* 2> null\n",
    "for dump_name in dump_names:\n",
    "    print(\"Loading {}...\".format(dump_name))\n",
    "    !wget -P $DATA_DIR/xml/ $DUMP_BASE_URL/$dump_name$dump_ext 2> /dev/null\n",
    "    !bzip2 -d $DATA_DIR/xml/$dump_name$dump_ext 2> /dev/null\n",
    "    \n",
    "print('Parsing XML + Fetching ORES...')\n",
    "process_dumps(XML_DIR, CSV_DIR, jupyter=True)\n",
    "!rm $DATA_DIR/xml/* 2> null\n",
    "print('Collected wiki dump(s) with ORES in {}/csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were built features that reflect diversity of text, sources and links:\n",
    "\n",
    "* Internal, external references count\n",
    "* Average number of references per block of text (Number of references / Number of paragraphs)\n",
    "* Citations count (Journals, Books, Web, News)\n",
    "* Number of images, files, etc in the articles\n",
    "* Number of non-approved references (“citation needed”)\n",
    "* Headings count (different levels)\n",
    "* ORES features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- Stub: double (nullable = true)\n",
      " |-- Start: double (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- GA: double (nullable = true)\n",
      " |-- FA: double (nullable = true)\n",
      " |-- n_words: double (nullable = false)\n",
      " |-- n_internal_links: double (nullable = false)\n",
      " |-- n_external_links: double (nullable = false)\n",
      " |-- level2: double (nullable = false)\n",
      " |-- level3: double (nullable = false)\n",
      " |-- level4: double (nullable = false)\n",
      " |-- level5: double (nullable = false)\n",
      " |-- level6: double (nullable = false)\n",
      " |-- book_citations: double (nullable = true)\n",
      " |-- journal_citations: double (nullable = true)\n",
      " |-- web_citations: double (nullable = true)\n",
      " |-- news_citations: double (nullable = true)\n",
      " |-- average_external_links: double (nullable = true)\n",
      " |-- average_internal_links: double (nullable = true)\n",
      " |-- n_paragraphs: double (nullable = false)\n",
      " |-- n_unreferenced: double (nullable = false)\n",
      " |-- n_images: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features = create_features(CSV_DIR, DATE, save=False)\n",
    "df_features.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a problem closely related to ORES but not exactly the same. Also ORES scores are not provided for every article, and certanly not every language. Thus we decided to use unsupervised learning techniques for data grouping. We use ORES scores as labels where it's available, and for the data without labels we search for the closest cluster and assign the most representative label of this cluster.\n",
    "\n",
    "We've chosen a **bisecting K-means** as a clustering algorithm for our data. The algorithm starts from a single cluster. Iteratively it finds divisible clusters on the bottom level and bisects each of them using k-means, until there are k leaf clusters in total or no leaf clusters are divisible.\n",
    "Among the key properties of the algorithm:\n",
    "\n",
    "\\+ hierarchical top-down approach\n",
    "\n",
    "\\+ parallelism\n",
    "\n",
    "\\+ high speed and efficiency (in terms of entropy, F measure and overall similarity) [1]\n",
    "\n",
    "\\- needs a hyperparameter k — fixed number of clusters — as input; this can be solved by maximizing the likelihood of the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've chosen a **Silhouette coefficient** to measure how appropriately data have been clustered.\n",
    "\n",
    "For each point a Silhouette coefficient $s(i)$ is calculated using:\n",
    "- mean intra-cluster distance $a(i)$\n",
    "\n",
    "\\begin{equation}\n",
    "a(i)=\\frac{1}{\\left|C_{i}\\right|-1} \\sum_{j \\in C_{i}, i \\neq j} d(i, j)\n",
    "\\end{equation}\n",
    "\n",
    "- mean nearest-cluster distance $b(i)$:\n",
    "\n",
    "\\begin{equation}\n",
    "b(i)=\\min _{k \\neq i} \\frac{1}{\\left|C_{k}\\right|} \\sum_{j \\in C_{k}} d(i, j)\n",
    "\\end{equation}\n",
    "\n",
    "- $a(i)$ and $b(i)$ are combined in the following way:\n",
    "\n",
    "\\begin{equation}\n",
    "s(i)=\\left\\{\\begin{array}{ll}{1-a(i) / b(i),} & {\\text { if } a(i)<b(i)} \\\\ {0,} & {\\text { if } a(i)=b(i)} \\\\ {b(i) / a(i)-1,} & {\\text { if } a(i)>b(i)}\\end{array}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "So the score $s(i)$ is between $-1$ and $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- the references distribution:\n",
      "  >   76% scientific papers (journals, publications, etc)\n",
      "  >   12% books\n",
      "  >   12% internet resources (news, archive, etc)\n",
      "  >    0% media materials (prints)\n",
      "- this article has a good amount of content and references\n"
     ]
    }
   ],
   "source": [
    "test_article(\"Principal component analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create supervised machine learning model with ORES as labels and our features as inputs. The resulted model should be transferred to the other languages that didn’t support by ORES (like Ukrainian) <br><br> \n",
    "\n",
    "* Test the PySpark MLP / Random Forest / Gradient boosting and get the features importance (visualize the pluses and minuses of the articles references) <br><br>\n",
    "\n",
    "* Fix clustering with ORES features <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Karypis, M.S.G., Kumar, V.: A comparison of document clustering techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
