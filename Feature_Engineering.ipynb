{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK_PATH = '../spark/spark-2.4.3-bin-hadoop2.7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init(PYSPARK_PATH)\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, size, UserDefinedFunction\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from functools import reduce\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_path = \"csv_data/top-10-sample-output.csv\"\n",
    "df_path = \"csv_data/sample-output-ores.csv\"\n",
    "df_out_path = \"{}_features.csv\".format(df_path[:-4])\n",
    "df = spark.read.csv(df_path, inferSchema=True, header=True, multiLine=True, escape='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA + Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns: ['_c0', 'Unnamed: 0', 'comment', 'contributor', 'format', 'id', 'ip', 'model', 'ns', 'parentid', 'restrictions', 'revision', 'sha1', 'text', 'timestamp', 'title', 'username', 'revid', 'B', 'C', 'FA', 'GA', 'Start', 'Stub']\n",
      "Unique values for..\n",
      "\t format : ['text/x-wiki']\n",
      "\t model : ['wikitext']\n",
      "\t ns : [12, 6, 4, 100, 118, 0]\n",
      "\t contributor : [None, '  ', ' ']\n",
      "\t revision : ['        ', '       ', '         ', '          ']\n",
      "\t restrictions : ['edit=autoconfirmed:move=sysop', 'edit=autoconfirmed:move=autoconfirmed', 'edit=sysop:move=sysop', None, 'move=:edit=', 'sysop', 'move=sysop', 'move=sysop:edit=sysop']\n",
      "Useful columns: ['sha1', 'timestamp', 'title', 'text', 'Stub', 'Start', 'C', 'B', 'GA', 'FA']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Columns filtering\n",
    "    Useful: sha1 (as identifier),  timestamp, title, text\n",
    "    Questionable: user, comment, ip, id (there are different articles with the same id), parentid, restrictions\n",
    "    Not useful (no unique info): model, format, ns, contributor, revision, restrictions\n",
    "\"\"\" \n",
    "\n",
    "print(\"All columns:\", df.columns)\n",
    "print(\"Unique values for..\")\n",
    "for column in [\"format\", \"model\", \"ns\", \"contributor\", \"revision\", \"restrictions\"]:\n",
    "    print(\"\\t\", column, \":\", df.select(column).distinct().rdd.map(lambda r: r[0]).collect())\n",
    "    \n",
    "ores_weights = {'Stub': 1, 'Start': 2, 'C': 3, 'B': 4, 'GA': 5, 'FA': 6}\n",
    "ores_scores = list(ores_weights.keys())\n",
    "useful_columns = [\"sha1\", \"timestamp\", \"title\", \"text\"] + ores_scores\n",
    "print(\"Useful columns:\", useful_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sha1: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- Stub: double (nullable = true)\n",
      " |-- Start: double (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- GA: double (nullable = true)\n",
      " |-- FA: double (nullable = true)\n",
      "\n",
      "Size of the DataFrame: 38842 records\n"
     ]
    }
   ],
   "source": [
    "clean_df = df[useful_columns]\n",
    "clean_df.printSchema()\n",
    "print(\"Size of the DataFrame: {} records\".format(clean_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|                sha1|          timestamp|      title|                text|                Stub|               Start|                  C|                  B|                 GA|                 FA|\n",
      "+--------------------+-------------------+-----------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|5ib24sew99ceyko33...|2019-04-11 16:19:39|The X-Files|{{About|the telev...|0.003073017103110...|0.007612710147196926|0.02019868704430509| 0.0510891495437035|0.31740231803291463| 0.6006241181287694|\n",
      "|q2nngvea32gwowk9y...|2019-04-25 23:53:46|Third World|{{mergefrom|Third...|0.006592174737638895| 0.03438149756654297|  0.452702637198141|0.20820061897204512| 0.2271170545728272|0.07100601695280477|\n",
      "|ihx6i3puongpsv8n4...|2019-04-21 05:46:04| Twin Peaks|{{About|the TV se...|0.003455710297672221|0.008565475704403651|0.04089799986525003|0.10020166311586297| 0.4847607192099156| 0.3621184318068957|\n",
      "|s8aob488ldyo67vob...|2019-04-25 14:51:44|   Thallium|{{distinguish|Thu...|0.004939667031234878|0.011896300734913129|0.06475218026328473| 0.1765290927233268| 0.4800378156349076|  0.261844943612333|\n",
      "+--------------------+-------------------+-----------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_counts(df):\n",
    "    return df.withColumn('n_words', size(split(col('text'), ' ')))\n",
    "\n",
    "df_features = words_counts(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Headings counting\n",
    "Syntaxis:\n",
    "    ==Level 2==\n",
    "    ===Level 3===\n",
    "    ====Level 4====\n",
    "    =====Level 5=====\n",
    "    ======Level 6======\n",
    "\"\"\"\n",
    "\n",
    "def single_head_level_count(text, level):\n",
    "    assert level in range(2,7)\n",
    "    pattern = \"=\" * level\n",
    "    pattern = pattern + \"[a-zA-Z0-9.,!? ]+\" + pattern\n",
    "    return size(split(text, pattern=pattern))-1\n",
    "\n",
    "def count_headings(df):\n",
    "    return reduce(\n",
    "        lambda df, level: df.withColumn(\"level{}\".format(level),\n",
    "                                        single_head_level_count(col(\"text\"), level)),\n",
    "        range(2,7), df)\n",
    "    \n",
    "df_features = count_headings(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Citation counting\n",
    "Syntaxis:\n",
    "    {{cite {book}(.*?)}}\n",
    "    {{cite {journal}(.*?)}}\n",
    "\"\"\"\n",
    "\n",
    "def citation_counter(citation_source):\n",
    "    def _count_citations(text):\n",
    "        matches = re.findall(f\"{{cite {citation_source}(.*?)}}\", text, re.IGNORECASE)\n",
    "        return len(matches)\n",
    "    return _count_citations\n",
    "\n",
    "book_citations_count = UserDefinedFunction(citation_counter(\"book\"), IntegerType())\n",
    "journal_citations_count = UserDefinedFunction(citation_counter(\"journal\"), IntegerType())\n",
    "\n",
    "df_features = df_features.withColumn(\"book_citations\", book_citations_count(\"text\"))\\\n",
    "  .withColumn(\"journal_citations\", journal_citations_count(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Internal Links:\n",
    "    [[A]] -- internal reference to an article titled A\n",
    "    [[A|B]] -- internal reference to an article titled A (written as B)\n",
    "    [[A#C|B]] -- internal reference to a section C of an article titled A (written as B)'''\n",
    "\n",
    "def count_internal_links(df):\n",
    "    \n",
    "    pattern = \"\\[\\[[a-zA-Z0-9.,!? ]+\\]\\]\"\n",
    "    pattern += \"|\\[\\[[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\]\\]\"\n",
    "    pattern += \"|\\[\\[[a-zA-Z0-9.,!? ]+#[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\]\\]\"\n",
    "    \n",
    "    return df.withColumn(\"n_internal_links\",\n",
    "                         size(split(col('text'), pattern=pattern))-1)\n",
    "\n",
    "df_features = count_internal_links(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''External Links:\n",
    "    https://www.google.com -- simple link\n",
    "    [https://www.google.com] -- link (reference)\n",
    "    [https://www.google.com A] -- reference written as A\n",
    "    <ref name=\"B\">[https://www.google.com A]</ref> -- reference A written as B, can be referenced again like:\n",
    "    <ref name=\"B\" /> -- reference to the source B\n",
    "    <ref>Lots of words</ref> -- reference without a link\n",
    "    {{sfnm|1a1=Craig|1y=2005|1p=14|2a1=Sheehan|2y=2003|2p=85}} -- external reference\n",
    "    Example:\n",
    "        {{sfnm|1a1=McLaughlin|1y=2007|1p=59|2a1=Flint|2y=2009|2p=27}} -- McLaughlin 2007, p. 59; Flint 2009, p. 27.\n",
    "        {{sfnm|1a1=Craig|1y=2005|1p=14|2a1=Sheehan|2y=2003|2p=85}} -- Craig 2005, p. 14; Sheehan 2003, p. 85.'''\n",
    "\n",
    "def count_external_links(df):\n",
    "    \n",
    "    pattern = 'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    pattern += '|\\[https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\]'\n",
    "    pattern += '|\\[https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\ [a-zA-Z0-9.,!? ]+]'\n",
    "    pattern += '<ref name=\"[a-zA-Z0-9.,!? ]+\">\\[https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\]'\n",
    "    \n",
    "    # template of the external reference\n",
    "    #template = '\\{\\{sfnm\\|1a1=[a-zA-Z]+\\|1y=[0-9]+\\|1p=[0-9]+\\|2a1=[a-zA-Z]+\\|2y=[0-9]+\\|2p=[0-9]+\\}\\}'\n",
    "    \n",
    "    # <ref name=\"B\" /> - this form use information from other reference, so we didn't count it again\n",
    "    # <ref>Lots of words</ref> - reference without a link will be in the other feature\n",
    "    \n",
    "    return df.withColumn(\"n_external_links\",\n",
    "                         size(split(col('text'), pattern=pattern))-1)\n",
    "\n",
    "df_features = count_external_links(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paragraphs\n",
    "\"\"\"\n",
    "\n",
    "def count_paragraphs(df):\n",
    "    \n",
    "    # filter the basic wikipedia syntaxis\n",
    "    pattern_filtering = '\\n\\n\\{\\{.*\\}\\}\\n\\n|\\n\\n\\[\\[.*\\]\\]\\n\\n|\\n\\n={1,7}.*={1,7}\\n\\n'\n",
    "    # split by two enters\n",
    "    pattern_splitting = '\\n\\n'\n",
    "\n",
    "    return df.withColumn('n_paragraphs', size(split(regexp_replace(col('text'), \n",
    "                                                                   pattern_filtering, ''), \n",
    "                                                    pattern_splitting))-1)\n",
    "\n",
    "df_features = count_paragraphs(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''<ref>Lots of words</ref> -- reference without a link\n",
    "{{cn}} -- citation needed'''\n",
    "\n",
    "def count_unreferenced(df):\n",
    "    \n",
    "    # citation needed and references without link\n",
    "    pattern = '\\{\\{cn\\}\\}|<ref>[a-zA-Z0-9.,!? ]+</ref>'\n",
    "    \n",
    "    return df.withColumn('n_unreferenced', size(split(col('text'), pattern))-1)\n",
    "\n",
    "df_features = count_unreferenced(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''[[Category:Category name]]\n",
    "[[:Category:Category name]]\n",
    "[[:File:File name]]'''\n",
    "\n",
    "def count_categories(df):\n",
    "    \n",
    "    #using template\n",
    "    pattern = '\\[\\[:?Category:[a-zA-Z0-9.,\\-!?\\(\\) ]+\\]\\]'\n",
    "    \n",
    "    return df.withColumn('n_categories', size(split(col('text'), pattern))-1)\n",
    "\n",
    "df_features = count_categories(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    [[File: | thumb  | upright | right | alt= | caption ]]\n",
    "'''\n",
    "\n",
    "def count_of_images(df):\n",
    "    any_text = \"[a-zA-Z0-9.,!? ]+ \\] \"\n",
    "    pattern = \"\\[[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\]\"\n",
    "    return df.withColumn(\"n_images\", size(split(col('text'), pattern=pattern))-1)\n",
    "\n",
    "df_features = count_of_images(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sha1: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- Stub: double (nullable = true)\n",
      " |-- Start: double (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- GA: double (nullable = true)\n",
      " |-- FA: double (nullable = true)\n",
      " |-- n_words: integer (nullable = false)\n",
      " |-- level2: integer (nullable = false)\n",
      " |-- level3: integer (nullable = false)\n",
      " |-- level4: integer (nullable = false)\n",
      " |-- level5: integer (nullable = false)\n",
      " |-- level6: integer (nullable = false)\n",
      " |-- book_citations: integer (nullable = true)\n",
      " |-- journal_citations: integer (nullable = true)\n",
      " |-- n_internal_links: integer (nullable = false)\n",
      " |-- n_external_links: integer (nullable = false)\n",
      " |-- n_paragraphs: integer (nullable = false)\n",
      " |-- n_unreferenced: integer (nullable = false)\n",
      " |-- n_categories: integer (nullable = false)\n",
      " |-- n_images: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = ['title',\n",
    "                  'Stub',\n",
    "                  'Start',\n",
    "                  'C',\n",
    "                  'B',\n",
    "                  'GA',\n",
    "                  'FA',\n",
    "                  'n_words',\n",
    "                  'level2',\n",
    "                  'level3',\n",
    "                  'level4',\n",
    "                  'level5',\n",
    "                  'level6',\n",
    "                  'book_citations',\n",
    "                  'journal_citations',\n",
    "                  'n_internal_links',\n",
    "                  'n_external_links',\n",
    "                  'n_paragraphs',\n",
    "                  'n_unreferenced',\n",
    "                  'n_categories',\n",
    "                  'n_images'\n",
    "                 ]\n",
    "\n",
    "df_features = df_features.select(list(map(lambda x: df_features[x].cast('double') if x != 'title' else df_features[x], \n",
    "                                          features_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features_names:\n",
    "    \n",
    "    df_features = df_features.filter(df_features[feature].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.toPandas().to_csv(df_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=features_names[1:],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df_features = vectorAssembler.setHandleInvalid(\"skip\").transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(df_features)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaled_features = scalerModel.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = scaled_features.select('scaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(scaled_features=DenseVector([0.0018, 0.0054, 0.0183, 0.0559, 0.3611, 0.6502, 0.4907, 0.1323, 0.107, 0.0617, 0.0, 0.0, 0.0821, 0.0, 0.2171, 0.2139, 0.1974, 0.0, 0.1747, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0054, 0.0371, 0.5141, 0.2426, 0.258, 0.0761, 0.0528, 0.0389, 0.0, 0.0, 0.0, 0.0, 0.0321, 0.0062, 0.0291, 0.0045, 0.0494, 0.0051, 0.0262, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0022, 0.0065, 0.0421, 0.1143, 0.5523, 0.3917, 0.267, 0.0934, 0.0576, 0.0, 0.0, 0.0, 0.025, 0.0531, 0.1191, 0.0839, 0.2017, 0.0, 0.2314, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0037, 0.0105, 0.0694, 0.2049, 0.5469, 0.283, 0.1301, 0.07, 0.0329, 0.0123, 0.0, 0.0, 0.0429, 0.0938, 0.0628, 0.0385, 0.0858, 0.0, 0.0087, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0046, 0.0304, 0.3329, 0.4951, 0.0672, 0.2051, 0.0748, 0.0389, 0.0, 0.0, 0.0, 0.0, 0.0107, 0.0031, 0.0524, 0.0136, 0.0515, 0.0, 0.0044, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0032, 0.0295, 0.6133, 0.3768, 0.0959, 0.0228, 0.0441, 0.0467, 0.0206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0197, 0.0113, 0.0601, 0.0, 0.0087, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0039, 0.0206, 0.3271, 0.3004, 0.1104, 0.3571, 0.1155, 0.0817, 0.0412, 0.0, 0.0, 0.0, 0.0107, 0.0062, 0.0445, 0.0106, 0.0794, 0.0, 0.0349, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0054, 0.0291, 0.2328, 0.2412, 0.2757, 0.3338, 0.1204, 0.0661, 0.0206, 0.0, 0.0, 0.0, 0.0036, 0.0, 0.0919, 0.0204, 0.0708, 0.0, 0.0306, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.002, 0.0188, 0.799, 0.2659, 0.0245, 0.0273, 0.082, 0.0311, 0.0123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0352, 0.0484, 0.0901, 0.0051, 0.0175, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0067, 0.5585, 0.4569, 0.1103, 0.0138, 0.0042, 0.0213, 0.0156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065, 0.0068, 0.015, 0.0, 0.0131, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0011, 0.0035, 0.0128, 0.0462, 0.1359, 0.8806, 0.436, 0.0817, 0.0535, 0.0, 0.0, 0.0, 0.0071, 0.0, 0.108, 0.0726, 0.1996, 0.0, 0.0524, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.9466, 0.0511, 0.005, 0.0096, 0.0012, 0.0011, 0.0001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004, 0.0, 0.0, 0.0, 0.0, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0053, 0.0159, 0.1656, 0.2191, 0.5014, 0.2154, 0.1042, 0.0428, 0.0123, 0.0, 0.0, 0.0, 0.05, 0.0344, 0.0327, 0.0128, 0.1974, 0.0204, 0.0044, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.9536, 0.0446, 0.0048, 0.0083, 0.0012, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004, 0.0, 0.0, 0.0, 0.0, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0027, 0.0079, 0.022, 0.0734, 0.7436, 0.2645, 0.1802, 0.0739, 0.0247, 0.0, 0.0, 0.0, 0.0179, 0.0219, 0.0732, 0.0733, 0.1137, 0.0204, 0.1485, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.004, 0.0908, 0.8146, 0.1729, 0.0453, 0.0096, 0.0513, 0.0623, 0.0247, 0.0, 0.0, 0.0, 0.0071, 0.0, 0.0312, 0.0008, 0.0815, 0.0, 0.0131, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0008, 0.0105, 0.7102, 0.402, 0.0113, 0.0085, 0.1457, 0.1051, 0.0658, 0.0, 0.0, 0.0, 0.0107, 0.0, 0.0506, 0.0227, 0.1502, 0.0051, 0.0175, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0056, 0.1797, 0.4225, 0.3783, 0.1456, 0.0148, 0.0451, 0.0195, 0.0, 0.0, 0.0, 0.0, 0.0071, 0.0094, 0.0459, 0.003, 0.0515, 0.0, 0.0044, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.0048, 0.2798, 0.5956, 0.2284, 0.028, 0.0087, 0.0156, 0.0195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0062, 0.0133, 0.0038, 0.0279, 0.0, 0.0218, 0.5])),\n",
       " Row(scaled_features=DenseVector([0.9801, 0.0195, 0.0034, 0.0043, 0.001, 0.001, 0.0002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0021, 0.0, 0.0, 0.5]))]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features.limit(20).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2703.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 195.0 failed 1 times, most recent failure: Lost task 0.0 in stage 195.0 (TID 7351, localhost, executor driver): java.lang.NullPointerException\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.mllib.clustering.BisectingKMeans$.org$apache$spark$mllib$clustering$BisectingKMeans$$summarize(BisectingKMeans.scala:304)\n\tat org.apache.spark.mllib.clustering.BisectingKMeans.run(BisectingKMeans.scala:171)\n\tat org.apache.spark.ml.clustering.BisectingKMeans$$anonfun$fit$1.apply(BisectingKMeans.scala:272)\n\tat org.apache.spark.ml.clustering.BisectingKMeans$$anonfun$fit$1.apply(BisectingKMeans.scala:257)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.clustering.BisectingKMeans.fit(BisectingKMeans.scala:257)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-436eb8396423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Trains a bisecting k-means model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBisectingKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scaled_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Evaluate clustering.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ucu/mmds/spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/Documents/ucu/mmds/spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ucu/mmds/spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m../spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ucu/mmds/spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m../spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2703.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 195.0 failed 1 times, most recent failure: Lost task 0.0 in stage 195.0 (TID 7351, localhost, executor driver): java.lang.NullPointerException\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.mllib.clustering.BisectingKMeans$.org$apache$spark$mllib$clustering$BisectingKMeans$$summarize(BisectingKMeans.scala:304)\n\tat org.apache.spark.mllib.clustering.BisectingKMeans.run(BisectingKMeans.scala:171)\n\tat org.apache.spark.ml.clustering.BisectingKMeans$$anonfun$fit$1.apply(BisectingKMeans.scala:272)\n\tat org.apache.spark.ml.clustering.BisectingKMeans$$anonfun$fit$1.apply(BisectingKMeans.scala:257)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.clustering.BisectingKMeans.fit(BisectingKMeans.scala:257)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n"
     ]
    }
   ],
   "source": [
    "# Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans(featuresCol='scaled_features').setK(3).setSeed(1)\n",
    "model = bkm.fit(scaled_features)\n",
    "\n",
    "# Evaluate clustering.\n",
    "cost = model.computeCost(scaled_features)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(cost))\n",
    "\n",
    "# Shows the result. \n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions and collect into the list\n",
    "labels = model.transform(df_features).select('prediction')\n",
    "labels = labels.collect() \n",
    "#create vanilla list with ints \n",
    "# instead of list with element type Rows\n",
    "labels = list(map(lambda x: x.prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA compressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df_features)\n",
    "\n",
    "compressed = model.transform(df_features).select(\"pcaFeatures\")\n",
    "compressed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = compressed.toPandas()\n",
    "compressed.pcaFeatures = compressed.pcaFeatures.apply(lambda x: np.array(x))\n",
    "compressed = compressed.pcaFeatures.values\n",
    "compressed = np.stack(compressed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {0: 'red', 1: 'green', 2: 'blue'}\n",
    "colors = list(map(lambda x: colors[x], labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
