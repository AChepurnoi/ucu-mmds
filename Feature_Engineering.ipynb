{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK_PATH = '../spark/spark-2.4.3-bin-hadoop2.7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init(PYSPARK_PATH)\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, size, UserDefinedFunction\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from functools import reduce\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_path = \"csv_data/top-10-sample-output.csv\"\n",
    "df_path = \"csv_data/sample-output-ores.csv\"\n",
    "df_out_path = \"{}_features.csv\".format(df_path[:-4])\n",
    "df = spark.read.csv(df_path, inferSchema=True, header=True, multiLine=True, escape='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA + Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns: ['_c0', 'Unnamed: 0', 'comment', 'contributor', 'format', 'id', 'ip', 'model', 'ns', 'parentid', 'restrictions', 'revision', 'sha1', 'text', 'timestamp', 'title', 'username', 'revid', 'B', 'C', 'FA', 'GA', 'Start', 'Stub']\n",
      "Unique values for..\n",
      "\t format : ['text/x-wiki']\n",
      "\t model : ['wikitext']\n",
      "\t ns : [12, 6, 4, 100, 118, 0]\n",
      "\t contributor : [None, '  ', ' ']\n",
      "\t revision : ['        ', '       ', '         ', '          ']\n",
      "\t restrictions : ['edit=autoconfirmed:move=sysop', 'edit=autoconfirmed:move=autoconfirmed', 'edit=sysop:move=sysop', None, 'move=:edit=', 'sysop', 'move=sysop', 'move=sysop:edit=sysop']\n",
      "Useful columns: ['sha1', 'timestamp', 'title', 'text', 'Stub', 'Start', 'C', 'B', 'GA', 'FA']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Columns filtering\n",
    "    Useful: sha1 (as identifier),  timestamp, title, text\n",
    "    Questionable: user, comment, ip, id (there are different articles with the same id), parentid, restrictions\n",
    "    Not useful (no unique info): model, format, ns, contributor, revision, restrictions\n",
    "\"\"\" \n",
    "\n",
    "print(\"All columns:\", df.columns)\n",
    "print(\"Unique values for..\")\n",
    "for column in [\"format\", \"model\", \"ns\", \"contributor\", \"revision\", \"restrictions\"]:\n",
    "    print(\"\\t\", column, \":\", df.select(column).distinct().rdd.map(lambda r: r[0]).collect())\n",
    "    \n",
    "ores_weights = {'Stub': 1, 'Start': 2, 'C': 3, 'B': 4, 'GA': 5, 'FA': 6}\n",
    "ores_scores = list(ores_weights.keys())\n",
    "useful_columns = [\"sha1\", \"timestamp\", \"title\", \"text\"] + ores_scores\n",
    "print(\"Useful columns:\", useful_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sha1: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- Stub: double (nullable = true)\n",
      " |-- Start: double (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- GA: double (nullable = true)\n",
      " |-- FA: double (nullable = true)\n",
      "\n",
      "Size of the DataFrame: 38842 records\n"
     ]
    }
   ],
   "source": [
    "clean_df = df[useful_columns]\n",
    "clean_df.printSchema()\n",
    "print(\"Size of the DataFrame: {} records\".format(clean_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|                sha1|          timestamp|      title|                text|                Stub|               Start|                  C|                  B|                 GA|                 FA|\n",
      "+--------------------+-------------------+-----------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|5ib24sew99ceyko33...|2019-04-11 16:19:39|The X-Files|{{About|the telev...|0.003073017103110...|0.007612710147196926|0.02019868704430509| 0.0510891495437035|0.31740231803291463| 0.6006241181287694|\n",
      "|q2nngvea32gwowk9y...|2019-04-25 23:53:46|Third World|{{mergefrom|Third...|0.006592174737638895| 0.03438149756654297|  0.452702637198141|0.20820061897204512| 0.2271170545728272|0.07100601695280477|\n",
      "|ihx6i3puongpsv8n4...|2019-04-21 05:46:04| Twin Peaks|{{About|the TV se...|0.003455710297672221|0.008565475704403651|0.04089799986525003|0.10020166311586297| 0.4847607192099156| 0.3621184318068957|\n",
      "|s8aob488ldyo67vob...|2019-04-25 14:51:44|   Thallium|{{distinguish|Thu...|0.004939667031234878|0.011896300734913129|0.06475218026328473| 0.1765290927233268| 0.4800378156349076|  0.261844943612333|\n",
      "+--------------------+-------------------+-----------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_counts(df):\n",
    "    return df.withColumn('n_words', size(split(col('text'), ' ')))\n",
    "\n",
    "df_features = words_counts(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Headings counting\n",
    "Syntaxis:\n",
    "    ==Level 2==\n",
    "    ===Level 3===\n",
    "    ====Level 4====\n",
    "    =====Level 5=====\n",
    "    ======Level 6======\n",
    "\"\"\"\n",
    "\n",
    "def single_head_level_count(text, level):\n",
    "    assert level in range(2,7)\n",
    "    pattern = \"=\" * level\n",
    "    pattern = pattern + \"[a-zA-Z0-9.,!? ]+\" + pattern\n",
    "    return size(split(text, pattern=pattern))-1\n",
    "\n",
    "def count_headings(df):\n",
    "    return reduce(\n",
    "        lambda df, level: df.withColumn(\"level{}\".format(level),\n",
    "                                        single_head_level_count(col(\"text\"), level)),\n",
    "        range(2,7), df)\n",
    "    \n",
    "df_features = count_headings(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Citation counting\n",
    "Syntaxis:\n",
    "    {{cite {book}(.*?)}}\n",
    "    {{cite {journal}(.*?)}}\n",
    "\"\"\"\n",
    "\n",
    "def citation_counter(citation_source):\n",
    "    def _count_citations(text):\n",
    "        matches = re.findall(f\"{{cite {citation_source}(.*?)}}\", text, re.IGNORECASE)\n",
    "        return len(matches)\n",
    "    return _count_citations\n",
    "\n",
    "book_citations_count = UserDefinedFunction(citation_counter(\"book\"), IntegerType())\n",
    "journal_citations_count = UserDefinedFunction(citation_counter(\"journal\"), IntegerType())\n",
    "\n",
    "df_features = df_features.withColumn(\"book_citations\", book_citations_count(\"text\"))\\\n",
    "  .withColumn(\"journal_citations\", journal_citations_count(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Internal Links:\n",
    "    [[A]] -- internal reference to an article titled A\n",
    "    [[A|B]] -- internal reference to an article titled A (written as B)\n",
    "    [[A#C|B]] -- internal reference to a section C of an article titled A (written as B)'''\n",
    "\n",
    "def count_internal_links(df):\n",
    "    \n",
    "    pattern = \"\\[\\[[a-zA-Z0-9.,!? ]+\\]\\]\"\n",
    "    pattern += \"|\\[\\[[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\]\\]\"\n",
    "    pattern += \"|\\[\\[[a-zA-Z0-9.,!? ]+#[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\]\\]\"\n",
    "    \n",
    "    return df.withColumn(\"n_internal_links\",\n",
    "                         size(split(col('text'), pattern=pattern))-1)\n",
    "\n",
    "df_features = count_internal_links(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''External Links:\n",
    "    https://www.google.com -- simple link\n",
    "    [https://www.google.com] -- link (reference)\n",
    "    [https://www.google.com A] -- reference written as A\n",
    "    <ref name=\"B\">[https://www.google.com A]</ref> -- reference A written as B, can be referenced again like:\n",
    "    <ref name=\"B\" /> -- reference to the source B\n",
    "    <ref>Lots of words</ref> -- reference without a link\n",
    "    {{sfnm|1a1=Craig|1y=2005|1p=14|2a1=Sheehan|2y=2003|2p=85}} -- external reference\n",
    "    Example:\n",
    "        {{sfnm|1a1=McLaughlin|1y=2007|1p=59|2a1=Flint|2y=2009|2p=27}} -- McLaughlin 2007, p. 59; Flint 2009, p. 27.\n",
    "        {{sfnm|1a1=Craig|1y=2005|1p=14|2a1=Sheehan|2y=2003|2p=85}} -- Craig 2005, p. 14; Sheehan 2003, p. 85.'''\n",
    "\n",
    "def count_external_links(df):\n",
    "    \n",
    "    pattern = 'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    pattern += '|\\[https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\]'\n",
    "    pattern += '|\\[https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\ [a-zA-Z0-9.,!? ]+]'\n",
    "    pattern += '<ref name=\"[a-zA-Z0-9.,!? ]+\">\\[https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\]'\n",
    "    \n",
    "    # template of the external reference\n",
    "    #template = '\\{\\{sfnm\\|1a1=[a-zA-Z]+\\|1y=[0-9]+\\|1p=[0-9]+\\|2a1=[a-zA-Z]+\\|2y=[0-9]+\\|2p=[0-9]+\\}\\}'\n",
    "    \n",
    "    # <ref name=\"B\" /> - this form use information from other reference, so we didn't count it again\n",
    "    # <ref>Lots of words</ref> - reference without a link will be in the other feature\n",
    "    \n",
    "    return df.withColumn(\"n_external_links\",\n",
    "                         size(split(col('text'), pattern=pattern))-1)\n",
    "\n",
    "df_features = count_external_links(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paragraphs\n",
    "\"\"\"\n",
    "\n",
    "def count_paragraphs(df):\n",
    "    \n",
    "    # filter the basic wikipedia syntaxis\n",
    "    pattern_filtering = '\\n\\n\\{\\{.*\\}\\}\\n\\n|\\n\\n\\[\\[.*\\]\\]\\n\\n|\\n\\n={1,7}.*={1,7}\\n\\n'\n",
    "    # split by two enters\n",
    "    pattern_splitting = '\\n\\n'\n",
    "\n",
    "    return df.withColumn('n_paragraphs', size(split(regexp_replace(col('text'), \n",
    "                                                                   pattern_filtering, ''), \n",
    "                                                    pattern_splitting))-1)\n",
    "\n",
    "df_features = count_paragraphs(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''<ref>Lots of words</ref> -- reference without a link\n",
    "{{cn}} -- citation needed'''\n",
    "\n",
    "def count_unreferenced(df):\n",
    "    \n",
    "    # citation needed and references without link\n",
    "    pattern = '\\{\\{cn\\}\\}|<ref>[a-zA-Z0-9.,!? ]+</ref>'\n",
    "    \n",
    "    return df.withColumn('n_unreferenced', size(split(col('text'), pattern))-1)\n",
    "\n",
    "df_features = count_unreferenced(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''[[Category:Category name]]\n",
    "[[:Category:Category name]]\n",
    "[[:File:File name]]'''\n",
    "\n",
    "def count_categories(df):\n",
    "    \n",
    "    #using template\n",
    "    pattern = '\\[\\[:?Category:[a-zA-Z0-9.,\\-!?\\(\\) ]+\\]\\]'\n",
    "    \n",
    "    return df.withColumn('n_categories', size(split(col('text'), pattern))-1)\n",
    "\n",
    "df_features = count_categories(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    [[File: | thumb  | upright | right | alt= | caption ]]\n",
    "'''\n",
    "\n",
    "def count_of_images(df):\n",
    "    any_text = \"[a-zA-Z0-9.,!? ]+ \\] \"\n",
    "    pattern = \"\\[[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\|[a-zA-Z0-9.,!? ]+\\]\"\n",
    "    return df.withColumn(\"n_images\", size(split(col('text'), pattern=pattern))-1)\n",
    "\n",
    "df_features = count_of_images(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sha1: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- Stub: double (nullable = true)\n",
      " |-- Start: double (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- GA: double (nullable = true)\n",
      " |-- FA: double (nullable = true)\n",
      " |-- n_words: integer (nullable = false)\n",
      " |-- level2: integer (nullable = false)\n",
      " |-- level3: integer (nullable = false)\n",
      " |-- level4: integer (nullable = false)\n",
      " |-- level5: integer (nullable = false)\n",
      " |-- level6: integer (nullable = false)\n",
      " |-- book_citations: integer (nullable = true)\n",
      " |-- journal_citations: integer (nullable = true)\n",
      " |-- n_internal_links: integer (nullable = false)\n",
      " |-- n_external_links: integer (nullable = false)\n",
      " |-- n_paragraphs: integer (nullable = false)\n",
      " |-- n_unreferenced: integer (nullable = false)\n",
      " |-- n_categories: integer (nullable = false)\n",
      " |-- n_images: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = ['title',\n",
    "                  'Stub',\n",
    "                  'Start',\n",
    "                  'C',\n",
    "                  'B',\n",
    "                  'GA',\n",
    "                  'FA',\n",
    "                  'n_words',\n",
    "                  'level2',\n",
    "                  'level3',\n",
    "                  'level4',\n",
    "                  'level5',\n",
    "                  'level6',\n",
    "                  'book_citations',\n",
    "                  'journal_citations',\n",
    "                  'n_internal_links',\n",
    "                  'n_external_links',\n",
    "                  'n_paragraphs',\n",
    "                  'n_unreferenced',\n",
    "                  'n_categories',\n",
    "                  'n_images'\n",
    "                 ]\n",
    "\n",
    "df_features = df_features.select(list(map(lambda x: df_features[x].cast('double') if x != 'title' else df_features[x], \n",
    "                                          features_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features_names:\n",
    "    \n",
    "    df_features = df_features.filter(df_features[feature].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.toPandas().to_csv(df_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=features_names[1:],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df_features = vectorAssembler.setHandleInvalid(\"skip\").transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.select('features').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0031, 0.0076, 0.0202, 0.0511, 0.3174, 0.6006, 19873.0, 34.0, 26.0, 5.0, 0.0, 0.0, 23.0, 0.0, 605.0, 283.0, 92.0, 0.0, 40.0, 0.0])),\n",
       " Row(features=DenseVector([0.0066, 0.0344, 0.4527, 0.2082, 0.2271, 0.071, 2140.0, 10.0, 0.0, 0.0, 0.0, 0.0, 9.0, 2.0, 81.0, 6.0, 23.0, 1.0, 6.0, 0.0])),\n",
       " Row(features=DenseVector([0.0035, 0.0086, 0.0409, 0.1002, 0.4848, 0.3621, 10813.0, 24.0, 14.0, 0.0, 0.0, 0.0, 7.0, 17.0, 332.0, 111.0, 94.0, 0.0, 53.0, 0.0])),\n",
       " Row(features=DenseVector([0.0049, 0.0119, 0.0648, 0.1765, 0.48, 0.2618, 5271.0, 18.0, 8.0, 1.0, 0.0, 0.0, 12.0, 30.0, 175.0, 51.0, 40.0, 0.0, 2.0, 0.0])),\n",
       " Row(features=DenseVector([0.0058, 0.0287, 0.2947, 0.4207, 0.0601, 0.19, 3030.0, 10.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 146.0, 18.0, 24.0, 0.0, 1.0, 0.0])),\n",
       " Row(features=DenseVector([0.0045, 0.028, 0.5393, 0.3212, 0.0852, 0.0219, 1789.0, 12.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 55.0, 15.0, 28.0, 0.0, 2.0, 0.0])),\n",
       " Row(features=DenseVector([0.0051, 0.0204, 0.2895, 0.2569, 0.0979, 0.3302, 4678.0, 21.0, 10.0, 0.0, 0.0, 0.0, 3.0, 2.0, 124.0, 14.0, 37.0, 0.0, 8.0, 0.0])),\n",
       " Row(features=DenseVector([0.0066, 0.0276, 0.2073, 0.207, 0.2427, 0.3088, 4876.0, 17.0, 5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 256.0, 27.0, 33.0, 0.0, 7.0, 0.0])),\n",
       " Row(features=DenseVector([0.0033, 0.0189, 0.7013, 0.2278, 0.0227, 0.026, 3324.0, 8.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 98.0, 64.0, 42.0, 1.0, 4.0, 0.0])),\n",
       " Row(features=SparseVector(20, {0: 0.0079, 1: 0.4744, 2: 0.4028, 3: 0.0969, 4: 0.0133, 5: 0.0047, 6: 865.0, 7: 4.0, 14: 18.0, 15: 9.0, 16: 7.0, 18: 3.0})),\n",
       " Row(features=DenseVector([0.0024, 0.006, 0.0153, 0.0429, 0.1202, 0.8131, 17661.0, 21.0, 13.0, 0.0, 0.0, 0.0, 2.0, 0.0, 301.0, 96.0, 93.0, 0.0, 12.0, 0.0])),\n",
       " Row(features=SparseVector(20, {0: 0.9289, 1: 0.0462, 2: 0.0086, 3: 0.0122, 4: 0.0023, 5: 0.0018, 6: 4.0, 14: 1.0})),\n",
       " Row(features=DenseVector([0.0065, 0.0165, 0.1487, 0.1885, 0.4403, 0.1995, 4223.0, 11.0, 3.0, 0.0, 0.0, 0.0, 14.0, 11.0, 91.0, 17.0, 92.0, 4.0, 1.0, 0.0])),\n",
       " Row(features=SparseVector(20, {0: 0.9358, 1: 0.0407, 2: 0.0084, 3: 0.0111, 4: 0.0023, 5: 0.0018, 6: 3.0, 14: 1.0})),\n",
       " Row(features=DenseVector([0.004, 0.0097, 0.0234, 0.0658, 0.6522, 0.2448, 7299.0, 19.0, 6.0, 0.0, 0.0, 0.0, 5.0, 7.0, 204.0, 97.0, 53.0, 4.0, 34.0, 0.0])),\n",
       " Row(features=DenseVector([0.0052, 0.0797, 0.7149, 0.1496, 0.0409, 0.0097, 2080.0, 16.0, 6.0, 0.0, 0.0, 0.0, 2.0, 0.0, 87.0, 1.0, 38.0, 0.0, 3.0, 0.0])),\n",
       " Row(features=DenseVector([0.0021, 0.0119, 0.6238, 0.3423, 0.0112, 0.0087, 5903.0, 27.0, 16.0, 0.0, 0.0, 0.0, 3.0, 0.0, 141.0, 30.0, 70.0, 1.0, 4.0, 0.0])),\n",
       " Row(features=DenseVector([0.0067, 0.1547, 0.3728, 0.3225, 0.1288, 0.0145, 1828.0, 5.0, 0.0, 0.0, 0.0, 0.0, 2.0, 3.0, 128.0, 4.0, 24.0, 0.0, 1.0, 0.0])),\n",
       " Row(features=DenseVector([0.006, 0.2391, 0.5238, 0.1963, 0.0258, 0.0089, 633.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 37.0, 5.0, 13.0, 0.0, 5.0, 0.0])),\n",
       " Row(features=SparseVector(20, {0: 0.9617, 1: 0.0195, 2: 0.0072, 3: 0.0077, 4: 0.0021, 5: 0.0018, 6: 10.0, 16: 1.0}))]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.select('features').limit(20).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2092.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 6127, localhost, executor driver): java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1127)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.mllib.clustering.BisectingKMeans$.org$apache$spark$mllib$clustering$BisectingKMeans$$summarize(BisectingKMeans.scala:304)\n\tat org.apache.spark.mllib.clustering.BisectingKMeans.run(BisectingKMeans.scala:171)\n\tat org.apache.spark.ml.clustering.BisectingKMeans$$anonfun$fit$1.apply(BisectingKMeans.scala:272)\n\tat org.apache.spark.ml.clustering.BisectingKMeans$$anonfun$fit$1.apply(BisectingKMeans.scala:257)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.clustering.BisectingKMeans.fit(BisectingKMeans.scala:257)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1127)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-da93b46dd8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Trains a bisecting k-means model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBisectingKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Evaluate clustering.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ucu/mmds/spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/Documents/ucu/mmds/spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ucu/mmds/spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m../spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ucu/mmds/spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m../spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2092.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 6127, localhost, executor driver): java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1127)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.mllib.clustering.BisectingKMeans$.org$apache$spark$mllib$clustering$BisectingKMeans$$summarize(BisectingKMeans.scala:304)\n\tat org.apache.spark.mllib.clustering.BisectingKMeans.run(BisectingKMeans.scala:171)\n\tat org.apache.spark.ml.clustering.BisectingKMeans$$anonfun$fit$1.apply(BisectingKMeans.scala:272)\n\tat org.apache.spark.ml.clustering.BisectingKMeans$$anonfun$fit$1.apply(BisectingKMeans.scala:257)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.clustering.BisectingKMeans.fit(BisectingKMeans.scala:257)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1127)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n"
     ]
    }
   ],
   "source": [
    "# Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans().setK(3).setSeed(1)\n",
    "model = bkm.fit(df_features)\n",
    "\n",
    "# Evaluate clustering.\n",
    "cost = model.computeCost(df_features)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(cost))\n",
    "\n",
    "# Shows the result.\n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions and collect into the list\n",
    "labels = model.transform(df_features).select('prediction')\n",
    "labels = labels.collect() \n",
    "#create vanilla list with ints \n",
    "# instead of list with element type Rows\n",
    "labels = list(map(lambda x: x.prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA compressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df_features)\n",
    "\n",
    "compressed = model.transform(df_features).select(\"pcaFeatures\")\n",
    "compressed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = compressed.toPandas()\n",
    "compressed.pcaFeatures = compressed.pcaFeatures.apply(lambda x: np.array(x))\n",
    "compressed = compressed.pcaFeatures.values\n",
    "compressed = np.stack(compressed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {0: 'red', 1: 'green', 2: 'blue'}\n",
    "colors = list(map(lambda x: colors[x], labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
